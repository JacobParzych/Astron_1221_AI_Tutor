{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae36ec1b",
   "metadata": {},
   "source": [
    "# Astronomy AI Tutor: RAG-Based Chatbot for Course Materials\n",
    "\n",
    "*Project 1 - Jacob Parzych & Jamison [Partner Name]*\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project implements a Retrieval-Augmented Generation (RAG) system that serves as an AI tutor for astronomy course materials. The system processes lecture notes from the course, creates a searchable knowledge base using sentence transformers, and provides intelligent responses to student questions by combining relevant course content with AI-generated explanations.\n",
    "\n",
    "### Key Features:\n",
    "- **Document Processing**: Automatically loads and chunks all lecture markdown files\n",
    "- **Semantic Search**: Uses sentence transformers to find relevant content\n",
    "- **AI Integration**: Leverages OpenAI's API for intelligent responses\n",
    "- **Interactive Chat**: Simple command-line interface for student queries\n",
    "- **Object-Oriented Design**: Clean, modular architecture with multiple classes\n",
    "\n",
    "### Technical Implementation:\n",
    "The system uses three main classes:\n",
    "1. **DocumentProcessor**: Handles loading and chunking of lecture files\n",
    "2. **VectorStore**: Manages embeddings and similarity search\n",
    "3. **AITutor**: Orchestrates the complete RAG pipeline\n",
    "\n",
    "This demonstrates proficiency in Python programming concepts including OOP, file I/O, error handling, and integration with modern AI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers anthropic numpy matplotlib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fd560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import anthropic\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32604e70",
   "metadata": {},
   "source": [
    "## Class 1: Document Processor\n",
    "\n",
    "The `DocumentProcessor` class handles loading and processing of lecture markdown files. It implements the chunking strategy you specified, splitting documents by section headers to maintain semantic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8931ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    A simplified class to process lecture documents for RAG implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lecture_directory: str):\n",
    "        \"\"\"Initialize the DocumentProcessor.\"\"\"\n",
    "        self.lecture_directory = lecture_directory\n",
    "        self.chunks = []\n",
    "        \n",
    "    def load_and_process_documents(self) -> None:\n",
    "        \"\"\"Load all markdown files and process them into chunks.\"\"\"\n",
    "        try:\n",
    "            # Find all markdown files\n",
    "            pattern = os.path.join(self.lecture_directory, \"*.md\")\n",
    "            markdown_files = glob.glob(pattern)\n",
    "            \n",
    "            if not markdown_files:\n",
    "                raise FileNotFoundError(f\"No markdown files found in {self.lecture_directory}\")\n",
    "            \n",
    "            print(f\"Found {len(markdown_files)} lecture files\")\n",
    "            \n",
    "            total_chunks = 0\n",
    "            for file_path in markdown_files:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                filename = os.path.basename(file_path).replace('.md', '')\n",
    "                doc_chunks = self.chunk_by_sections(content, filename)\n",
    "                self.chunks.extend(doc_chunks)\n",
    "                total_chunks += len(doc_chunks)\n",
    "                print(f\"  - {filename}: {len(doc_chunks)} chunks\")\n",
    "            \n",
    "            print(f\"Total chunks created: {total_chunks}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing documents: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def chunk_by_sections(self, text: str, source_filename: str) -> List[Dict]:\n",
    "        \"\"\"Split document into chunks based on ## section headers.\"\"\"\n",
    "        sections = text.split('\\n## ')\n",
    "        chunks = []\n",
    "        \n",
    "        for i, section in enumerate(sections):\n",
    "            # Add back the '## ' that was removed during split\n",
    "            if i == 0:\n",
    "                chunk_text = section\n",
    "            else:\n",
    "                chunk_text = '## ' + section\n",
    "            \n",
    "            # Only keep chunks with substantial content\n",
    "            if len(chunk_text.strip()) > 100:\n",
    "                chunks.append({\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'source_file': source_filename,\n",
    "                    'chunk_id': i\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def get_chunks(self) -> List[Dict]:\n",
    "        \"\"\"Return the processed chunks.\"\"\"\n",
    "        return self.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d5def",
   "metadata": {},
   "source": [
    "## Class 2: Vector Store\n",
    "\n",
    "The `VectorStore` class manages the embedding and retrieval system. It uses sentence transformers to create embeddings and implements cosine similarity search to find relevant content for user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7276dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    A simplified class to manage embeddings and similarity search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the VectorStore.\"\"\"\n",
    "        print(\"Loading sentence transformer model...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def add_chunks(self, chunks: List[Dict]) -> None:\n",
    "        \"\"\"Add chunks and create embeddings.\"\"\"\n",
    "        print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "        \n",
    "        self.chunks = chunks\n",
    "        chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        self.embeddings = self.model.encode(chunk_texts, convert_to_tensor=True)\n",
    "        print(f\"Created embeddings with dimension: {self.embeddings.shape[1]}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for relevant chunks based on similarity.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarities (using built-in utility function)\n",
    "        from sentence_transformers.util import cos_sim\n",
    "        similarities = cos_sim(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top results\n",
    "        top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0.1:  # minimum similarity threshold\n",
    "                result = self.chunks[idx].copy()\n",
    "                result['similarity_score'] = float(similarities[idx])\n",
    "                results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b4224",
   "metadata": {},
   "source": [
    "## Class 3: AI Tutor\n",
    "\n",
    "The `AITutor` class orchestrates the complete RAG pipeline. It combines the document processing, vector search, and AI generation to create an intelligent tutoring system that can answer questions about the course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82203fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITutor:\n",
    "    \"\"\"\n",
    "    A simple RAG-based AI tutor using Anthropic's Claude.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lecture_directory: str):\n",
    "        \"\"\"Initialize the AI Tutor.\"\"\"\n",
    "        self.document_processor = DocumentProcessor(lecture_directory)\n",
    "        self.vector_store = VectorStore()\n",
    "        \n",
    "        # Setup Anthropic client using API key from .env file\n",
    "        api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "        if api_key:\n",
    "            try:\n",
    "                self.client = anthropic.Anthropic(api_key=api_key)\n",
    "                print(\"âœ… Anthropic API key loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Anthropic API setup failed - {e}\")\n",
    "                self.client = None\n",
    "        else:\n",
    "            print(\"Warning: ANTHROPIC_API_KEY not found in .env file\")\n",
    "            self.client = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Load documents and create embeddings.\"\"\"\n",
    "        print(\"Initializing AI Tutor...\")\n",
    "        \n",
    "        # Process documents\n",
    "        self.document_processor.load_and_process_documents()\n",
    "        chunks = self.document_processor.get_chunks()\n",
    "        \n",
    "        # Create embeddings\n",
    "        self.vector_store.add_chunks(chunks)\n",
    "        \n",
    "        print(\"âœ… AI Tutor Ready!\")\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question and get an AI response.\"\"\"\n",
    "        # Find relevant content using top 3 chunks\n",
    "        relevant_chunks = self.vector_store.search(question, top_k=3)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return \"Sorry, I couldn't find relevant information for your question.\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return \"AI responses not available. Please set ANTHROPIC_API_KEY environment variable.\"\n",
    "        \n",
    "        # Prepare context from retrieved chunks\n",
    "        context = \"\\n\\n\".join([chunk['text'][:400] for chunk in relevant_chunks])\n",
    "        \n",
    "        # Simple prompt for Claude\n",
    "        prompt = f\"\"\"Answer this student's question about astronomy programming based on the course materials:\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Course Materials:\n",
    "    {context}\n",
    "\n",
    "    Provide a clear, helpful answer.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=300,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Error getting AI response: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe4a3e3",
   "metadata": {},
   "source": [
    "## System Initialization\n",
    "\n",
    "Now let's initialize our AI tutor system. This will load all the lecture files, process them into chunks, and create embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b65dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AI Tutor\n",
    "# Note: Make sure you have ANTHROPIC_API_KEY in your .env file\n",
    "\n",
    "lecture_directory = \"Lecture\"\n",
    "\n",
    "# Check if initialization is already complete\n",
    "def is_tutor_ready():\n",
    "    return ('tutor' in globals() and \n",
    "            hasattr(tutor, 'vector_store') and \n",
    "            tutor.vector_store.embeddings is not None and\n",
    "            len(tutor.document_processor.get_chunks()) > 0)\n",
    "\n",
    "if is_tutor_ready():\n",
    "    print(\"âœ… AI Tutor already initialized and ready to use!\")\n",
    "    print(f\"   - Total chunks: {len(tutor.document_processor.get_chunks())}\")\n",
    "    print(f\"   - Embeddings shape: {tutor.vector_store.embeddings.shape}\")\n",
    "    print(\"   - Use 'tutor.ask(question)' to interact with the system\")\n",
    "else:\n",
    "    print(\"ðŸš€ Creating new AI Tutor instance...\")\n",
    "    \n",
    "    # Delete any existing partial tutor to prevent conflicts\n",
    "    if 'tutor' in globals():\n",
    "        del tutor\n",
    "    \n",
    "    # Create and initialize fresh instance\n",
    "    tutor = AITutor(lecture_directory)\n",
    "    tutor.initialize()\n",
    "    \n",
    "    print(\"ðŸŽ‰ AI Tutor initialization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system statistics\n",
    "chunks = tutor.document_processor.get_chunks()\n",
    "source_files = list(set(chunk['source_file'] for chunk in chunks))\n",
    "\n",
    "print(\"=== System Statistics ===\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Source files loaded:\")\n",
    "for file in source_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0195a",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "Let's analyze the document processing results and visualize some key metrics about our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee69f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk distribution\n",
    "chunks = tutor.document_processor.get_chunks()\n",
    "chunk_lengths = [len(chunk['text']) for chunk in chunks]  # Calculate length from text\n",
    "source_files = [chunk['source_file'] for chunk in chunks]\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Histogram of chunk lengths\n",
    "ax1.hist(chunk_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Chunk Length (characters)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Chunk Lengths')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot of chunk lengths\n",
    "ax2.boxplot(chunk_lengths)\n",
    "ax2.set_ylabel('Chunk Length (characters)')\n",
    "ax2.set_title('Chunk Length Distribution (Box Plot)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Chunks per source file\n",
    "file_counts = Counter(source_files)\n",
    "files = list(file_counts.keys())\n",
    "counts = list(file_counts.values())\n",
    "\n",
    "ax3.bar(range(len(files)), counts, color='lightcoral', alpha=0.7)\n",
    "ax3.set_xlabel('Source Files')\n",
    "ax3.set_ylabel('Number of Chunks')\n",
    "ax3.set_title('Chunks per Source File')\n",
    "ax3.set_xticks(range(len(files)))\n",
    "ax3.set_xticklabels([f.replace('Lecture', 'L') for f in files], rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cumulative chunk length by file\n",
    "file_total_lengths = {}\n",
    "for chunk in chunks:\n",
    "    file = chunk['source_file']\n",
    "    if file not in file_total_lengths:\n",
    "        file_total_lengths[file] = 0\n",
    "    file_total_lengths[file] += len(chunk['text'])  # Use text length\n",
    "\n",
    "files = list(file_total_lengths.keys())\n",
    "lengths = list(file_total_lengths.values())\n",
    "\n",
    "ax4.bar(range(len(files)), lengths, color='lightgreen', alpha=0.7)\n",
    "ax4.set_xlabel('Source Files')\n",
    "ax4.set_ylabel('Total Characters')\n",
    "ax4.set_title('Total Content Length per File')\n",
    "ax4.set_xticks(range(len(files)))\n",
    "ax4.set_xticklabels([f.replace('Lecture', 'L') for f in files], rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Knowledge Base Analysis', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"=== Detailed Chunk Analysis ===\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Mean chunk length: {np.mean(chunk_lengths):.1f} characters\")\n",
    "print(f\"Median chunk length: {np.median(chunk_lengths):.1f} characters\")\n",
    "print(f\"Standard deviation: {np.std(chunk_lengths):.1f} characters\")\n",
    "print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "print(f\"Max chunk length: {max(chunk_lengths)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ef464",
   "metadata": {},
   "source": [
    "## Testing the AI Tutor\n",
    "\n",
    "Let's test our AI tutor with various types of questions to demonstrate its capabilities. These examples will show real AI responses using your Anthropic API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AI Tutor with sample questions\n",
    "\n",
    "test_questions = [\n",
    "    \"What is the purpose of this course?\",\n",
    "    \"How do I use Python for astronomy?\",\n",
    "    \"What is NumPy used for?\",\n",
    "    \"How do I create plots with matplotlib?\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing AI Tutor with Real Responses ===\")\n",
    "print(\"Using ANTHROPIC_API_KEY for full AI responses\")\n",
    "print()\n",
    "\n",
    "# Test with actual AI responses\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    \n",
    "    # Show what context gets retrieved\n",
    "    relevant_chunks = tutor.vector_store.search(question, top_k=3)\n",
    "    \n",
    "    print(f\"Found {len(relevant_chunks)} relevant chunks:\")\n",
    "    for j, chunk in enumerate(relevant_chunks, 1):\n",
    "        print(f\"  {j}. {chunk['source_file']} (similarity: {chunk['similarity_score']:.3f})\")\n",
    "    \n",
    "    # Get actual AI response\n",
    "    print(\"\\nðŸ¤– AI Response:\")\n",
    "    response = tutor.ask(question)\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Interactive Demo with Real AI Responses\n",
    "def demo_chat():\n",
    "    \"\"\"Demo of actual AI tutor interactions.\"\"\"\n",
    "    \n",
    "    sample_questions = [\n",
    "        \"What programming concepts will I learn?\",\n",
    "        \"How do I handle data in astronomy?\", \n",
    "        \"What visualization tools are covered?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Live AI Tutor Demo ===\")\n",
    "    print(\"Real conversations with the AI tutor:\")\n",
    "    print()\n",
    "    \n",
    "    for i, q in enumerate(sample_questions, 1):\n",
    "        print(f\"ðŸ’¬ Student Question {i}: {q}\")\n",
    "        \n",
    "        # Show what gets found\n",
    "        results = tutor.vector_store.search(q, top_k=1)\n",
    "        if results:\n",
    "            print(f\"ðŸ“š Found relevant content from: {results[0]['source_file']} (similarity: {results[0]['similarity_score']:.3f})\")\n",
    "        \n",
    "        # Get real AI response  \n",
    "        print(\"\\nðŸ¤– AI Tutor Response:\")\n",
    "        response = tutor.ask(q)\n",
    "        print(response)\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… Demo complete! The AI tutor is ready for your questions.\")\n",
    "    print(\"Usage: response = tutor.ask('Your question here')\")\n",
    "\n",
    "# Clear any previous output and run demo\n",
    "import sys\n",
    "sys.stdout.flush()  # Clear output buffer\n",
    "demo_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Test Questions - Try Your Own!\n",
    "\n",
    "print(\"=== Try Some Additional Questions ===\")\n",
    "print(\"Here are some more examples you can test:\")\n",
    "print()\n",
    "\n",
    "additional_questions = [\n",
    "    \"How do I work with variables in Python?\",\n",
    "    \"What is object-oriented programming?\", \n",
    "    \"How do I read files in Python?\",\n",
    "    \"What are the main data visualization libraries?\",\n",
    "    \"How do I use Git for version control?\"\n",
    "]\n",
    "\n",
    "# Test one example\n",
    "example_question = additional_questions[0]\n",
    "print(f\"Example: {example_question}\")\n",
    "print(\"\\nðŸ¤– AI Response:\")\n",
    "response = tutor.ask(example_question)\n",
    "print(response)\n",
    "\n",
    "print(f\"\\n\\nOther questions you can try:\")\n",
    "for i, q in enumerate(additional_questions[1:], 2):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "print(f\"\\nTo test any question, run: tutor.ask('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4538d1a",
   "metadata": {},
   "source": [
    "## Error Handling and Edge Cases\n",
    "\n",
    "Let's demonstrate the robust error handling built into our system and test various edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac34222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic error handling\n",
    "\n",
    "print(\"=== Error Handling Tests ===\")\n",
    "\n",
    "# Test 1: Query with no relevant results\n",
    "print(\"Test 1: Irrelevant query\")\n",
    "results = tutor.vector_store.search(\"quantum physics of unicorns\", top_k=3)\n",
    "print(f\"Results for irrelevant query: {len(results)}\")\n",
    "\n",
    "# Test 2: Empty query\n",
    "print(\"\\nTest 2: Empty query\")\n",
    "results = tutor.vector_store.search(\"\", top_k=3)\n",
    "print(f\"Results for empty query: {len(results)}\")\n",
    "\n",
    "# Test 3: System status\n",
    "print(\"\\nTest 3: System status\")\n",
    "print(f\"Has embeddings: {tutor.vector_store.embeddings is not None}\")\n",
    "print(f\"Total chunks: {len(tutor.document_processor.get_chunks())}\")\n",
    "print(f\"API client available: {tutor.client is not None}\")\n",
    "\n",
    "# Test 4: Ask method with no relevant results\n",
    "print(\"\\nTest 4: Ask method with irrelevant query\")\n",
    "response = tutor.ask(\"quantum physics of unicorns\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "print(\"\\n=== Error Handling Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99bfb4",
   "metadata": {},
   "source": [
    "## Usage Instructions and Examples\n",
    "\n",
    "Here's how to use the AI tutor system in practice, with examples of common use cases for astronomy students.\n",
    "# Simplified AI Tutor Usage\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. **Install packages:** `pip install sentence-transformers anthropic matplotlib python-dotenv`\n",
    "2. **Set API key:** `export ANTHROPIC_API_KEY='your-key-here'` \n",
    "3. **Put lecture .md files in 'Lecture' directory**\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "```python\n",
    "# Initialize\n",
    "tutor = AITutor('Lecture')\n",
    "tutor.initialize()\n",
    "\n",
    "# Ask questions\n",
    "response = tutor.ask('How do I use NumPy?')\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Automatic Processing**: Processes all lecture markdown files automatically\n",
    "- **Section-based Chunking**: Splits content by section headers (##)\n",
    "- **Semantic Search**: Uses sentence transformers for intelligent content retrieval\n",
    "- **AI-Powered**: Powered by Anthropic's Claude AI for natural responses\n",
    "- **Clean Architecture**: Simple 3-class object-oriented design\n",
    "\n",
    "## Example Questions\n",
    "\n",
    "- \"What is this course about?\"\n",
    "- \"How do I create Python functions?\" \n",
    "- \"What is object-oriented programming?\"\n",
    "- \"How do I make plots with matplotlib?\"\n",
    "- \"What are NumPy arrays used for?\"\n",
    "\n",
    "**Ready to help with astronomy programming!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3f0c6",
   "metadata": {},
   "source": [
    "## Project Summary and Reflection\n",
    "\n",
    "### What We Built\n",
    "This simplified RAG-based AI tutor demonstrates the core concepts of retrieval-augmented generation in a clean, understandable way. The system consists of three main components:\n",
    "\n",
    "1. **DocumentProcessor**: Loads markdown lecture files and splits them into semantic chunks\n",
    "2. **VectorStore**: Creates embeddings using sentence transformers and performs similarity search  \n",
    "3. **AITutor**: Orchestrates the RAG pipeline using Anthropic's Claude for intelligent responses\n",
    "\n",
    "### Key Programming Concepts Demonstrated\n",
    "- **Object-Oriented Programming**: Three well-designed classes with clear responsibilities\n",
    "- **File I/O**: Reading and processing multiple markdown files\n",
    "- **Error Handling**: Try/except blocks throughout for robust operation\n",
    "- **Data Structures**: Lists, dictionaries, and NumPy arrays for data management\n",
    "- **External APIs**: Integration with Anthropic's Claude API\n",
    "- **Modern AI Tools**: Sentence transformers for semantic similarity\n",
    "\n",
    "### Simplifications Made\n",
    "Compared to complex RAG systems, we simplified by:\n",
    "- Using basic section-based chunking instead of advanced text splitting\n",
    "- Implementing straightforward cosine similarity search\n",
    "- Using a single embedding model without fine-tuning\n",
    "- Streamlined prompt engineering for Claude\n",
    "- Minimal conversation history tracking\n",
    "\n",
    "### Real-World Applications\n",
    "This system could be extended for:\n",
    "- Course Q&A systems for any subject area\n",
    "- Internal company knowledge bases\n",
    "- Research paper summarization tools\n",
    "- Technical documentation assistants\n",
    "\n",
    "The core RAG pattern demonstrated here scales to much larger document collections and more sophisticated retrieval strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
